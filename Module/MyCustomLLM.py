from pydantic import Field
from langchain.llms.base import LLM
from typing import Any, List, Mapping, Optional
from langchain.llms import GPT4All
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory
from langchain.chains import ConversationChain


class MyCustomLLM(LLM):
    """
    A custom LLM class that integrates gpt4all models

    Arguments:

    model_folder_path: (str) Folder path where the model lies
    allow_download: (bool) whether to download the model or not

    backend: (str) The backend of the model (Supported backends: llama/gptj)
    n_threads: (str) The number of threads to use
    n_predict: (str) The maximum numbers of tokens to generate
    temp: (str) Temperature to use for sampling
    top_p: (float) The top-p value to use for sampling
    top_k: (float) The top k values use for sampling
    n_batch: (int) Batch size for prompt processing
    repeat_last_n: (int) Last n number of tokens to penalize
    repeat_penalty: (float) The penalty to apply repeated tokens

    """
    model_name: str = Field(None, alias='model_name')
    model_folder_path: str = Field(None, alias='model_folder_path')
    allow_download: bool = Field(None, alias='allow_download')

    # # all the optional arguments
    backend: Optional[str] = 'llama'
    temp: Optional[float] = 0.7
    top_p: Optional[float] = 0.1
    top_k: Optional[int] = 40
    n_batch: Optional[int] = 8
    n_threads: Optional[int] = 4
    n_predict: Optional[int] = 256
    max_tokens: Optional[int] = 200
    repeat_last_n: Optional[int] = 64
    repeat_penalty: Optional[float] = 1.18

    # initialize the model
    gpt4_model_instance: Any = None
    memory = ConversationBufferMemory()
    conversation: Any = None

    def __init__(self, model_name, model_folder_path, allow_download, **kwargs):
        super(MyCustomLLM, self).__init__()
        self.model_name = model_name
        self.model_folder_path: str = model_folder_path
        self.allow_download = allow_download

        # trigger auto download
        # self.auto_download()

        self.gpt4_model_instance = GPT4All(
            model=self.model_folder_path + self.model_name,
            backend=self.backend,
            verbose=True
        )

        self.conversation = ConversationChain(
            llm=self.gpt4_model_instance,
            memory=self.memory,
            # verbose=True
        )

    @property
    def _llm_type(self) -> str:
        return 'custom'

    @property
    def _get_model_default_parameters(self):
        return {
            "max_tokens": self.max_tokens,
            "n_predict": self.n_predict,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "temp": self.temp,
            "n_batch": self.n_batch,
            "repeat_penalty": self.repeat_penalty,
            "repeat_last_n": self.repeat_last_n,
        }

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """
        Get all the identifying parameters
        """
        return {
            'model_name': self.model_name,
            'model_path': self.model_folder_path,
            'model_parameters': self._get_model_default_parameters
        }

    def _call(
            self,
            prompt: str, stop: Optional[List[str]] = None,
            run_manager: Optional[CallbackManagerForLLMRun] = None,
            **kwargs) -> str:
        """
        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered

        Returns:
            The string generated by the model
        """

        params = {
            **self._get_model_default_parameters,
            **kwargs
        }

        response = self.conversation.predict(input=prompt)
        return response
